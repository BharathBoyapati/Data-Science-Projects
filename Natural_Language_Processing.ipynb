{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Reading in the Yelp Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohan\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob, Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read yelp.csv into a DataFrame\n",
    "url = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/yelp.csv' \n",
    "yelp = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new DataFrame that only contains the 5-star and 1-star reviews\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "\n",
    "# define X and y\n",
    "X = yelp_best_worst.text\n",
    "y = yelp_best_worst.stars\n",
    "\n",
    "# split the new DataFrame into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use CountVectorizer to create document-term matrices from X_train and X_test\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3014x16825 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 234123 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_train_dtm[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1022, 16825)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 16825)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rows are documents, columns are terms (aka \"tokens\" or \"features\")\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yyyyy', 'z11', 'za', 'zabba', 'zach', 'zam', 'zanella', 'zankou', 'zappos', 'zatsiki', 'zen', 'zero', 'zest', 'zexperience', 'zha', 'zhou', 'zia', 'zihuatenejo', 'zilch', 'zin', 'zinburger', 'zinburgergeist', 'zinc', 'zinfandel', 'zing', 'zip', 'zipcar', 'zipper', 'zippers', 'zipps', 'ziti', 'zoe', 'zombi', 'zombies', 'zone', 'zones', 'zoning', 'zoo', 'zoyo', 'zucca', 'zucchini', 'zuchinni', 'zumba', 'zupa', 'zuzu', 'zwiebel', 'zzed', 'éclairs', 'école', 'ém']\n"
     ]
    }
   ],
   "source": [
    "# last 50 features\n",
    "print (vect.get_feature_names()[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show vectorizer options\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CountVectorizer documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 20838)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# don't convert to lowercase\n",
    "vect = CountVectorizer(lowercase=False)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CountVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 169847)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# include 1-grams and 2-grams\n",
    "vect = CountVectorizer(ngram_range=(1, 2))\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'zone out', u'zone when', u'zones', u'zones dolls', u'zoning', u'zoning issues', u'zoo', u'zoo and', u'zoo is', u'zoo not', u'zoo the', u'zoo ve', u'zoyo', u'zoyo for', u'zucca', u'zucca appetizer', u'zucchini', u'zucchini and', u'zucchini bread', u'zucchini broccoli', u'zucchini carrots', u'zucchini fries', u'zucchini pieces', u'zucchini strips', u'zucchini veal', u'zucchini very', u'zucchini with', u'zuchinni', u'zuchinni again', u'zuchinni the', u'zumba', u'zumba class', u'zumba or', u'zumba yogalates', u'zupa', u'zupa flavors', u'zuzu', u'zuzu in', u'zuzu is', u'zuzu the', u'zwiebel', u'zwiebel kr\\xe4uter', u'zzed', u'zzed in', u'\\xe9clairs', u'\\xe9clairs napoleons', u'\\xe9cole', u'\\xe9cole len\\xf4tre', u'\\xe9m', u'\\xe9m all']\n"
     ]
    }
   ],
   "source": [
    "# last 50 features\n",
    "print vect.get_feature_names()[-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicting the star rating:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.918786692759\n"
     ]
    }
   ],
   "source": [
    "# use default options for CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# create document-term matrices\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "# use Naive Bayes to predict the star rating\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "# calculate accuracy\n",
    "print metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81996086105675148"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate null accuracy\n",
    "y_test_binary = np.where(y_test==5, 1, 0)\n",
    "max(y_test_binary.mean(), 1 - y_test_binary.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that accepts a vectorizer and calculates the accuracy\n",
    "def tokenize_test(vect):\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    print ('Features: ', X_train_dtm.shape[1])\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    y_pred_class = nb.predict(X_test_dtm)\n",
    "    print ('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  8783\n",
      "Accuracy:  0.924657534247\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 2-grams\n",
    "vect = CountVectorizer(ngram_range=(1, 1), min_df=2)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show vectorizer options\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **stop_words:** string {'english'}, list, or None (default)\n",
    "- If 'english', a built-in stop word list for English is used.\n",
    "- If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "- If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  16528\n",
      "Accuracy:  0.915851272016\n"
     ]
    }
   ],
   "source": [
    "# remove English stop words\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'once', 'down', 'always', 're', 'to', 'formerly', 'first', 'each', 'via', 'most', 'con', 'indeed', 'not', 'also', 'noone', 'bottom', 'part', 'my', 'few', 'put', 'below', 'thru', 'up', 'would', 'forty', 'between', 'co', 'can', 'this', 'upon', 'was', 'please', 'least', 'otherwise', 'never', 'hereafter', 'call', 'cannot', 'themselves', 'without', 'along', 'rather', 'several', 'before', 'moreover', 'serious', 'whose', 'if', 'therein', 'same', 'find', 'whereupon', 'with', 'nothing', 'herein', 'have', 'under', 'latter', 'either', 'full', 'other', 'afterwards', 'eight', 'has', 'thick', 'couldnt', 'amount', 'un', 'for', 'what', 'describe', 'side', 'must', 'thin', 'two', 'many', 'back', 'ltd', 'inc', 'now', 'be', 'even', 'cant', 'been', 'were', 'becoming', 'onto', 'others', 'amongst', 'after', 'her', 'hasnt', 'where', 'go', 'latterly', 'name', 'nevertheless', 'until', 'although', 'such', 'seems', 'against', 'there', 'eleven', 'sometimes', 'here', 'in', 'beyond', 'next', 'only', 'therefore', 'top', 'all', 'detail', 'that', 'bill', 'thus', 'whereby', 'whither', 'six', 'get', 'none', 'due', 'during', 'from', 'toward', 'mill', 'while', 'fill', 'see', 'seeming', 'too', 'yet', 'than', 'namely', 'yourselves', 'give', 'behind', 'an', 'and', 'sincere', 'any', 'already', 'whether', 'becomes', 'thereby', 'more', 'interest', 'move', 'three', 'we', 'through', 'could', 'else', 'nobody', 'seemed', 'anywhere', 'you', 'will', 'of', 'twenty', 'one', 'almost', 'however', 'hundred', 'no', 'whence', 'hereby', 'seem', 'thereupon', 'whatever', 'do', 'within', 'ie', 'whenever', 'are', 'ours', 'every', 'mostly', 'myself', 'done', 'yours', 'made', 'him', 'why', 'except', 'your', 'which', 'whereas', 'his', 'by', 'had', 'out', 'sixty', 'show', 'wherever', 'me', 'eg', 'hers', 'as', 'besides', 'though', 'how', 'system', 'about', 'nine', 'twelve', 'someone', 'might', 'de', 'whereafter', 'our', 'over', 'both', 'a', 'anyhow', 'i', 'some', 'something', 'they', 'whoever', 'since', 'anyway', 'anything', 'but', 'third', 'fifty', 'whole', 'among', 'whom', 'across', 'again', 'alone', 'throughout', 'five', 'everyone', 'together', 'beforehand', 'into', 'may', 'on', 'it', 'is', 'further', 'when', 'still', 'nor', 'who', 'cry', 'he', 'am', 'should', 'per', 'then', 'empty', 'himself', 'because', 'ourselves', 'its', 'towards', 'fifteen', 'enough', 'everything', 'hereupon', 'being', 'she', 'became', 'above', 'us', 'wherein', 'ever', 'meanwhile', 'at', 'thereafter', 'fire', 'etc', 'become', 'nowhere', 'own', 'around', 'less', 'them', 'the', 'neither', 'last', 'so', 'four', 'thence', 'hence', 'often', 'somehow', 'very', 'much', 'found', 'sometime', 'yourself', 'elsewhere', 'beside', 'ten', 'amoungst', 'herself', 'well', 'or', 'off', 'itself', 'those', 'their', 'former', 'mine', 'front', 'everywhere', 'anyone', 'take', 'these', 'somewhere', 'keep', 'another', 'perhaps'})\n"
     ]
    }
   ],
   "source": [
    "# set of stop words\n",
    "print (vect.get_stop_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Other CountVectorizer Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  100\n",
      "Accuracy:  0.869863013699\n"
     ]
    }
   ],
   "source": [
    "# remove English stop words and only keep 100 features\n",
    "vect = CountVectorizer(stop_words='english', max_features=100)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'amazing', u'area', u'atmosphere', u'awesome', u'bad', u'bar', u'best', u'better', u'big', u'came', u'cheese', u'chicken', u'clean', u'coffee', u'come', u'day', u'definitely', u'delicious', u'did', u'didn', u'dinner', u'don', u'eat', u'excellent', u'experience', u'favorite', u'feel', u'food', u'free', u'fresh', u'friendly', u'friends', u'going', u'good', u'got', u'great', u'happy', u'home', u'hot', u'hour', u'just', u'know', u'like', u'little', u'll', u'location', u'long', u'looking', u'lot', u'love', u'lunch', u'make', u'meal', u'menu', u'minutes', u'need', u'new', u'nice', u'night', u'order', u'ordered', u'people', u'perfect', u'phoenix', u'pizza', u'place', u'pretty', u'prices', u'really', u'recommend', u'restaurant', u'right', u'said', u'salad', u'sandwich', u'sauce', u'say', u'service', u'staff', u'store', u'sure', u'table', u'thing', u'things', u'think', u'time', u'times', u'took', u'town', u'tried', u'try', u've', u'wait', u'want', u'way', u'went', u'wine', u'work', u'worth', u'years']\n"
     ]
    }
   ],
   "source": [
    "# all 100 features\n",
    "print vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  100000\n",
      "Accuracy:  0.885518590998\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 2-grams, and limit the number of features\n",
    "vect = CountVectorizer(ngram_range=(1, 2), max_features=100000)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  43957\n",
      "Accuracy:  0.932485322896\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 2-grams, and only include terms that appear at least 2 times\n",
    "vect = CountVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Introduction to TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob: \"Simplified Text Processing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\n",
      "\n",
      "Do yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\n",
      "\n",
      "While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\n",
      "\n",
      "Anyway, I can't wait to go back!\n"
     ]
    }
   ],
   "source": [
    "print (yelp_best_worst.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\n",
      "\n",
      "Do yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\n",
      "\n",
      "While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\n",
      "\n",
      "Anyway, I can't wait to go back!\n"
     ]
    }
   ],
   "source": [
    "# print the first review\n",
    "print (yelp_best_worst.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save it as a TextBlob object\n",
    "review = TextBlob(yelp_best_worst.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_cmpkey', '_compare', '_create_sentence_objects', '_strkey', 'analyzer', 'classifier', 'classify', 'correct', 'detect_language', 'ends_with', 'endswith', 'find', 'format', 'index', 'join', 'json', 'lower', 'ngrams', 'noun_phrases', 'np_counts', 'np_extractor', 'parse', 'parser', 'polarity', 'pos_tagger', 'pos_tags', 'raw', 'raw_sentences', 'replace', 'rfind', 'rindex', 'sentences', 'sentiment', 'serialized', 'split', 'starts_with', 'startswith', 'string', 'strip', 'stripped', 'subjectivity', 'tags', 'title', 'to_json', 'tokenize', 'tokenizer', 'tokens', 'translate', 'translator', 'upper', 'word_counts', 'words']\n"
     ]
    }
   ],
   "source": [
    "print (dir(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WordList(['My', 'wife']), WordList(['wife', 'took']), WordList(['took', 'me']), WordList(['me', 'here']), WordList(['here', 'on']), WordList(['on', 'my']), WordList(['my', 'birthday']), WordList(['birthday', 'for']), WordList(['for', 'breakfast']), WordList(['breakfast', 'and']), WordList(['and', 'it']), WordList(['it', 'was']), WordList(['was', 'excellent']), WordList(['excellent', 'The']), WordList(['The', 'weather']), WordList(['weather', 'was']), WordList(['was', 'perfect']), WordList(['perfect', 'which']), WordList(['which', 'made']), WordList(['made', 'sitting']), WordList(['sitting', 'outside']), WordList(['outside', 'overlooking']), WordList(['overlooking', 'their']), WordList(['their', 'grounds']), WordList(['grounds', 'an']), WordList(['an', 'absolute']), WordList(['absolute', 'pleasure']), WordList(['pleasure', 'Our']), WordList(['Our', 'waitress']), WordList(['waitress', 'was']), WordList(['was', 'excellent']), WordList(['excellent', 'and']), WordList(['and', 'our']), WordList(['our', 'food']), WordList(['food', 'arrived']), WordList(['arrived', 'quickly']), WordList(['quickly', 'on']), WordList(['on', 'the']), WordList(['the', 'semi-busy']), WordList(['semi-busy', 'Saturday']), WordList(['Saturday', 'morning']), WordList(['morning', 'It']), WordList(['It', 'looked']), WordList(['looked', 'like']), WordList(['like', 'the']), WordList(['the', 'place']), WordList(['place', 'fills']), WordList(['fills', 'up']), WordList(['up', 'pretty']), WordList(['pretty', 'quickly']), WordList(['quickly', 'so']), WordList(['so', 'the']), WordList(['the', 'earlier']), WordList(['earlier', 'you']), WordList(['you', 'get']), WordList(['get', 'here']), WordList(['here', 'the']), WordList(['the', 'better']), WordList(['better', 'Do']), WordList(['Do', 'yourself']), WordList(['yourself', 'a']), WordList(['a', 'favor']), WordList(['favor', 'and']), WordList(['and', 'get']), WordList(['get', 'their']), WordList(['their', 'Bloody']), WordList(['Bloody', 'Mary']), WordList(['Mary', 'It']), WordList(['It', 'was']), WordList(['was', 'phenomenal']), WordList(['phenomenal', 'and']), WordList(['and', 'simply']), WordList(['simply', 'the']), WordList(['the', 'best']), WordList(['best', 'I']), WordList(['I', \"'ve\"]), WordList([\"'ve\", 'ever']), WordList(['ever', 'had']), WordList(['had', 'I']), WordList(['I', \"'m\"]), WordList([\"'m\", 'pretty']), WordList(['pretty', 'sure']), WordList(['sure', 'they']), WordList(['they', 'only']), WordList(['only', 'use']), WordList(['use', 'ingredients']), WordList(['ingredients', 'from']), WordList(['from', 'their']), WordList(['their', 'garden']), WordList(['garden', 'and']), WordList(['and', 'blend']), WordList(['blend', 'them']), WordList(['them', 'fresh']), WordList(['fresh', 'when']), WordList(['when', 'you']), WordList(['you', 'order']), WordList(['order', 'it']), WordList(['it', 'It']), WordList(['It', 'was']), WordList(['was', 'amazing']), WordList(['amazing', 'While']), WordList(['While', 'EVERYTHING']), WordList(['EVERYTHING', 'on']), WordList(['on', 'the']), WordList(['the', 'menu']), WordList(['menu', 'looks']), WordList(['looks', 'excellent']), WordList(['excellent', 'I']), WordList(['I', 'had']), WordList(['had', 'the']), WordList(['the', 'white']), WordList(['white', 'truffle']), WordList(['truffle', 'scrambled']), WordList(['scrambled', 'eggs']), WordList(['eggs', 'vegetable']), WordList(['vegetable', 'skillet']), WordList(['skillet', 'and']), WordList(['and', 'it']), WordList(['it', 'was']), WordList(['was', 'tasty']), WordList(['tasty', 'and']), WordList(['and', 'delicious']), WordList(['delicious', 'It']), WordList(['It', 'came']), WordList(['came', 'with']), WordList(['with', '2']), WordList(['2', 'pieces']), WordList(['pieces', 'of']), WordList(['of', 'their']), WordList(['their', 'griddled']), WordList(['griddled', 'bread']), WordList(['bread', 'with']), WordList(['with', 'was']), WordList(['was', 'amazing']), WordList(['amazing', 'and']), WordList(['and', 'it']), WordList(['it', 'absolutely']), WordList(['absolutely', 'made']), WordList(['made', 'the']), WordList(['the', 'meal']), WordList(['meal', 'complete']), WordList(['complete', 'It']), WordList(['It', 'was']), WordList(['was', 'the']), WordList(['the', 'best']), WordList(['best', 'toast']), WordList(['toast', 'I']), WordList(['I', \"'ve\"]), WordList([\"'ve\", 'ever']), WordList(['ever', 'had']), WordList(['had', 'Anyway']), WordList(['Anyway', 'I']), WordList(['I', 'ca']), WordList(['ca', \"n't\"]), WordList([\"n't\", 'wait']), WordList(['wait', 'to']), WordList(['to', 'go']), WordList(['go', 'back'])]\n"
     ]
    }
   ],
   "source": [
    "print (review.ngrams(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.40246913580246907, subjectivity=0.6591122868900646)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['My', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'was', 'excellent', 'The', 'weather', 'was', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', 'grounds', 'an', 'absolute', 'pleasure', 'Our', 'waitress', 'was', 'excellent', 'and', 'our', 'food', 'arrived', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', 'looked', 'like', 'the', 'place', 'fills', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', 'was', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', 'had', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', 'ingredients', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', 'was', 'amazing', 'While', 'EVERYTHING', 'on', 'the', 'menu', 'looks', 'excellent', 'I', 'had', 'the', 'white', 'truffle', 'scrambled', 'eggs', 'vegetable', 'skillet', 'and', 'it', 'was', 'tasty', 'and', 'delicious', 'It', 'came', 'with', '2', 'pieces', 'of', 'their', 'griddled', 'bread', 'with', 'was', 'amazing', 'and', 'it', 'absolutely', 'made', 'the', 'meal', 'complete', 'It', 'was', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', 'had', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the words\n",
    "review.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"My wife took me here on my birthday for breakfast and it was excellent.\"),\n",
       " Sentence(\"The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.\"),\n",
       " Sentence(\"Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.\"),\n",
       " Sentence(\"It looked like the place fills up pretty quickly so the earlier you get here the better.\"),\n",
       " Sentence(\"Do yourself a favor and get their Bloody Mary.\"),\n",
       " Sentence(\"It was phenomenal and simply the best I've ever had.\"),\n",
       " Sentence(\"I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.\"),\n",
       " Sentence(\"It was amazing.\"),\n",
       " Sentence(\"While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.\"),\n",
       " Sentence(\"It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.\"),\n",
       " Sentence(\"It was the best \"toast\" I've ever had.\"),\n",
       " Sentence(\"Anyway, I can't wait to go back!\")]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the sentences\n",
    "review.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"my wife took me here on my birthday for breakfast and it was excellent.  the weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  our waitress was excellent and our food arrived quickly on the semi-busy saturday morning.  it looked like the place fills up pretty quickly so the earlier you get here the better.\n",
       "\n",
       "do yourself a favor and get their bloody mary.  it was phenomenal and simply the best i've ever had.  i'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  it was amazing.\n",
       "\n",
       "while everything on the menu looks excellent, i had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  it came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  it was the best \"toast\" i've ever had.\n",
       "\n",
       "anyway, i can't wait to go back!\")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some string methods are available\n",
    "review.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk as nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AbstractLazySequence', 'AffixTagger', 'AlignedSent', 'Alignment', 'AnnotationTask', 'ApplicationExpression', 'Assignment', 'BigramAssocMeasures', 'BigramCollocationFinder', 'BigramTagger', 'BinaryMaxentFeatureEncoding', 'BlanklineTokenizer', 'BllipParser', 'BottomUpChartParser', 'BottomUpLeftCornerChartParser', 'BottomUpProbabilisticChartParser', 'Boxer', 'BrillTagger', 'BrillTaggerTrainer', 'CFG', 'CRFTagger', 'CfgReadingCommand', 'ChartParser', 'ChunkParserI', 'ChunkScore', 'ClassifierBasedPOSTagger', 'ClassifierBasedTagger', 'ClassifierI', 'ConcordanceIndex', 'ConditionalExponentialClassifier', 'ConditionalFreqDist', 'ConditionalProbDist', 'ConditionalProbDistI', 'ConfusionMatrix', 'ContextIndex', 'ContextTagger', 'ContingencyMeasures', 'CoreNLPDependencyParser', 'CoreNLPParser', 'Counter', 'CrossValidationProbDist', 'DRS', 'DecisionTreeClassifier', 'DefaultTagger', 'DependencyEvaluator', 'DependencyGrammar', 'DependencyGraph', 'DependencyProduction', 'DictionaryConditionalProbDist', 'DictionaryProbDist', 'DiscourseTester', 'DrtExpression', 'DrtGlueReadingCommand', 'ELEProbDist', 'EarleyChartParser', 'Expression', 'FStructure', 'FeatDict', 'FeatList', 'FeatStruct', 'FeatStructReader', 'Feature', 'FeatureBottomUpChartParser', 'FeatureBottomUpLeftCornerChartParser', 'FeatureChartParser', 'FeatureEarleyChartParser', 'FeatureIncrementalBottomUpChartParser', 'FeatureIncrementalBottomUpLeftCornerChartParser', 'FeatureIncrementalChartParser', 'FeatureIncrementalTopDownChartParser', 'FeatureTopDownChartParser', 'FreqDist', 'HTTPPasswordMgrWithDefaultRealm', 'HeldoutProbDist', 'HiddenMarkovModelTagger', 'HiddenMarkovModelTrainer', 'HunposTagger', 'IBMModel', 'IBMModel1', 'IBMModel2', 'IBMModel3', 'IBMModel4', 'IBMModel5', 'ISRIStemmer', 'ImmutableMultiParentedTree', 'ImmutableParentedTree', 'ImmutableProbabilisticMixIn', 'ImmutableProbabilisticTree', 'ImmutableTree', 'IncrementalBottomUpChartParser', 'IncrementalBottomUpLeftCornerChartParser', 'IncrementalChartParser', 'IncrementalLeftCornerChartParser', 'IncrementalTopDownChartParser', 'Index', 'InsideChartParser', 'JSONTaggedDecoder', 'JSONTaggedEncoder', 'KneserNeyProbDist', 'LancasterStemmer', 'LaplaceProbDist', 'LazyConcatenation', 'LazyEnumerate', 'LazyIteratorList', 'LazyMap', 'LazySubsequence', 'LazyZip', 'LeftCornerChartParser', 'LidstoneProbDist', 'LineTokenizer', 'LogicalExpressionException', 'LongestChartParser', 'MLEProbDist', 'MWETokenizer', 'Mace', 'MaceCommand', 'MaltParser', 'MaxentClassifier', 'Model', 'MultiClassifierI', 'MultiParentedTree', 'MutableProbDist', 'NaiveBayesClassifier', 'NaiveBayesDependencyScorer', 'NgramAssocMeasures', 'NgramTagger', 'NonprojectiveDependencyParser', 'Nonterminal', 'OrderedDict', 'PCFG', 'Paice', 'ParallelProverBuilder', 'ParallelProverBuilderCommand', 'ParentedTree', 'ParserI', 'PerceptronTagger', 'PhraseTable', 'PorterStemmer', 'PositiveNaiveBayesClassifier', 'ProbDistI', 'ProbabilisticDependencyGrammar', 'ProbabilisticMixIn', 'ProbabilisticNonprojectiveParser', 'ProbabilisticProduction', 'ProbabilisticProjectiveDependencyParser', 'ProbabilisticTree', 'Production', 'ProjectiveDependencyParser', 'Prover9', 'Prover9Command', 'ProxyBasicAuthHandler', 'ProxyDigestAuthHandler', 'ProxyHandler', 'PunktSentenceTokenizer', 'QuadgramCollocationFinder', 'RSLPStemmer', 'RTEFeatureExtractor', 'RUS_PICKLE', 'RandomChartParser', 'RangeFeature', 'ReadingCommand', 'RecursiveDescentParser', 'RegexpChunkParser', 'RegexpParser', 'RegexpStemmer', 'RegexpTagger', 'RegexpTokenizer', 'ReppTokenizer', 'ResolutionProver', 'ResolutionProverCommand', 'SExprTokenizer', 'SLASH', 'Senna', 'SennaChunkTagger', 'SennaNERTagger', 'SennaTagger', 'SequentialBackoffTagger', 'ShiftReduceParser', 'SimpleGoodTuringProbDist', 'SklearnClassifier', 'SlashFeature', 'SnowballStemmer', 'SpaceTokenizer', 'StackDecoder', 'StanfordNERTagger', 'StanfordPOSTagger', 'StanfordSegmenter', 'StanfordTagger', 'StanfordTokenizer', 'StemmerI', 'SteppingChartParser', 'SteppingRecursiveDescentParser', 'SteppingShiftReduceParser', 'TYPE', 'TabTokenizer', 'TableauProver', 'TableauProverCommand', 'TaggerI', 'TestGrammar', 'Text', 'TextCat', 'TextCollection', 'TextTilingTokenizer', 'TnT', 'TokenSearcher', 'ToktokTokenizer', 'TopDownChartParser', 'TransitionParser', 'Tree', 'TreebankWordTokenizer', 'Trie', 'TrigramAssocMeasures', 'TrigramCollocationFinder', 'TrigramTagger', 'TweetTokenizer', 'TypedMaxentFeatureEncoding', 'Undefined', 'UniformProbDist', 'UnigramTagger', 'UnsortedChartParser', 'Valuation', 'Variable', 'ViterbiParser', 'WekaClassifier', 'WhitespaceTokenizer', 'WittenBellProbDist', 'WordNetLemmatizer', 'WordPunctTokenizer', '__author__', '__author_email__', '__builtins__', '__cached__', '__classifiers__', '__copyright__', '__doc__', '__file__', '__keywords__', '__license__', '__loader__', '__longdescr__', '__maintainer__', '__maintainer_email__', '__name__', '__package__', '__path__', '__spec__', '__url__', '__version__', 'absolute_import', 'accuracy', 'add_logs', 'agreement', 'align', 'alignment_error_rate', 'aline', 'api', 'app', 'apply_features', 'approxrand', 'arity', 'association', 'bigrams', 'binary_distance', 'binary_search_file', 'binding_ops', 'bisect', 'blankline_tokenize', 'bleu', 'bleu_score', 'bllip', 'boolean_ops', 'boxer', 'bracket_parse', 'breadth_first', 'brill', 'brill_trainer', 'build_opener', 'call_megam', 'casual', 'casual_tokenize', 'ccg', 'chain', 'chart', 'chat', 'choose', 'chunk', 'class_types', 'classify', 'clause', 'clean_html', 'clean_url', 'cluster', 'collections', 'collocations', 'combinations', 'compat', 'config_java', 'config_megam', 'config_weka', 'conflicts', 'confusionmatrix', 'conllstr2tree', 'conlltags2tree', 'corenlp', 'corpus', 'crf', 'custom_distance', 'data', 'decisiontree', 'decorator', 'decorators', 'defaultdict', 'demo', 'dependencygraph', 'deque', 'discourse', 'distance', 'download', 'download_gui', 'download_shell', 'downloader', 'draw', 'drt', 'earleychart', 'edit_distance', 'elementtree_indent', 'entropy', 'equality_preds', 'evaluate', 'evaluate_sents', 'everygrams', 'extract_rels', 'extract_test_sentences', 'f_measure', 'featstruct', 'featurechart', 'filestring', 'find', 'flatten', 'fractional_presence', 'getproxies', 'ghd', 'glue', 'grammar', 'guess_encoding', 'help', 'hmm', 'hunpos', 'ibm1', 'ibm2', 'ibm3', 'ibm4', 'ibm5', 'ibm_model', 'ieerstr2tree', 'improved_close_quote_regex', 'improved_open_quote_regex', 'improved_punct_regex', 'in_idle', 'induce_pcfg', 'inference', 'infile', 'inspect', 'install_opener', 'internals', 'interpret_sents', 'interval_distance', 'invert_dict', 'invert_graph', 'is_rel', 'islice', 'isri', 'jaccard_distance', 'json_tags', 'jsontags', 'lancaster', 'lazyimport', 'lfg', 'line_tokenize', 'linearlogic', 'load', 'load_parser', 'locale', 'log_likelihood', 'logic', 'mace', 'malt', 'map_tag', 'mapping', 'masi_distance', 'maxent', 'megam', 'memoize', 'metrics', 'misc', 'mwe', 'naivebayes', 'ne_chunk', 'ne_chunk_sents', 'ngrams', 'nltk.corpus', 'nonprojectivedependencyparser', 'nonterminals', 'numpy', 'os', 'pad_sequence', 'paice', 'parse', 'parse_sents', 'pchart', 'perceptron', 'pk', 'porter', 'pos_tag', 'pos_tag_sents', 'positivenaivebayes', 'pprint', 'pr', 'precision', 'presence', 'print_function', 'print_string', 'probability', 'projectivedependencyparser', 'prover9', 'punkt', 'py25', 'py26', 'py27', 'pydoc', 'python_2_unicode_compatible', 'raise_unorderable_types', 'ranks_from_scores', 'ranks_from_sequence', 're', 're_show', 'read_grammar', 'read_logic', 'read_valuation', 'recall', 'recursivedescent', 'regexp', 'regexp_span_tokenize', 'regexp_tokenize', 'register_tag', 'relextract', 'repp', 'resolution', 'ribes', 'ribes_score', 'root_semrep', 'rslp', 'rte_classifier', 'rte_classify', 'rte_features', 'rtuple', 'scikitlearn', 'scores', 'segmentation', 'sem', 'senna', 'sent_tokenize', 'sequential', 'set2rel', 'set_proxy', 'sexpr', 'sexpr_tokenize', 'shiftreduce', 'simple', 'sinica_parse', 'skipgrams', 'skolemize', 'slice_bounds', 'snowball', 'spearman', 'spearman_correlation', 'stack_decoder', 'stanford', 'stanford_segmenter', 'stem', 'str2tuple', 'string_span_tokenize', 'string_types', 'subprocess', 'subsumes', 'sum_logs', 'sys', 'tableau', 'tadm', 'tag', 'tagset_mapping', 'tagstr2tree', 'tbl', 'text', 'text_type', 'textcat', 'texttiling', 'textwrap', 'tkinter', 'tnt', 'tokenize', 'tokenwrap', 'toktok', 'toolbox', 'total_ordering', 'transitionparser', 'transitive_closure', 'translate', 'tree', 'tree2conllstr', 'tree2conlltags', 'treebank', 'treetransforms', 'trigrams', 'tuple2str', 'types', 'unify', 'unique_list', 'untag', 'usage', 'util', 'version_file', 'version_info', 'viterbi', 'weka', 'windowdiff', 'word_tokenize', 'wordnet', 'wordpunct_tokenize', 'wsd']\n"
     ]
    }
   ],
   "source": [
    "print (dir(nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['My', 'wife']),\n",
       " WordList(['wife', 'took']),\n",
       " WordList(['took', 'me']),\n",
       " WordList(['me', 'here']),\n",
       " WordList(['here', 'on']),\n",
       " WordList(['on', 'my']),\n",
       " WordList(['my', 'birthday']),\n",
       " WordList(['birthday', 'for']),\n",
       " WordList(['for', 'breakfast']),\n",
       " WordList(['breakfast', 'and']),\n",
       " WordList(['and', 'it']),\n",
       " WordList(['it', 'was']),\n",
       " WordList(['was', 'excellent']),\n",
       " WordList(['excellent', 'The']),\n",
       " WordList(['The', 'weather']),\n",
       " WordList(['weather', 'was']),\n",
       " WordList(['was', 'perfect']),\n",
       " WordList(['perfect', 'which']),\n",
       " WordList(['which', 'made']),\n",
       " WordList(['made', 'sitting']),\n",
       " WordList(['sitting', 'outside']),\n",
       " WordList(['outside', 'overlooking']),\n",
       " WordList(['overlooking', 'their']),\n",
       " WordList(['their', 'grounds']),\n",
       " WordList(['grounds', 'an']),\n",
       " WordList(['an', 'absolute']),\n",
       " WordList(['absolute', 'pleasure']),\n",
       " WordList(['pleasure', 'Our']),\n",
       " WordList(['Our', 'waitress']),\n",
       " WordList(['waitress', 'was']),\n",
       " WordList(['was', 'excellent']),\n",
       " WordList(['excellent', 'and']),\n",
       " WordList(['and', 'our']),\n",
       " WordList(['our', 'food']),\n",
       " WordList(['food', 'arrived']),\n",
       " WordList(['arrived', 'quickly']),\n",
       " WordList(['quickly', 'on']),\n",
       " WordList(['on', 'the']),\n",
       " WordList(['the', 'semi-busy']),\n",
       " WordList(['semi-busy', 'Saturday']),\n",
       " WordList(['Saturday', 'morning']),\n",
       " WordList(['morning', 'It']),\n",
       " WordList(['It', 'looked']),\n",
       " WordList(['looked', 'like']),\n",
       " WordList(['like', 'the']),\n",
       " WordList(['the', 'place']),\n",
       " WordList(['place', 'fills']),\n",
       " WordList(['fills', 'up']),\n",
       " WordList(['up', 'pretty']),\n",
       " WordList(['pretty', 'quickly']),\n",
       " WordList(['quickly', 'so']),\n",
       " WordList(['so', 'the']),\n",
       " WordList(['the', 'earlier']),\n",
       " WordList(['earlier', 'you']),\n",
       " WordList(['you', 'get']),\n",
       " WordList(['get', 'here']),\n",
       " WordList(['here', 'the']),\n",
       " WordList(['the', 'better']),\n",
       " WordList(['better', 'Do']),\n",
       " WordList(['Do', 'yourself']),\n",
       " WordList(['yourself', 'a']),\n",
       " WordList(['a', 'favor']),\n",
       " WordList(['favor', 'and']),\n",
       " WordList(['and', 'get']),\n",
       " WordList(['get', 'their']),\n",
       " WordList(['their', 'Bloody']),\n",
       " WordList(['Bloody', 'Mary']),\n",
       " WordList(['Mary', 'It']),\n",
       " WordList(['It', 'was']),\n",
       " WordList(['was', 'phenomenal']),\n",
       " WordList(['phenomenal', 'and']),\n",
       " WordList(['and', 'simply']),\n",
       " WordList(['simply', 'the']),\n",
       " WordList(['the', 'best']),\n",
       " WordList(['best', 'I']),\n",
       " WordList(['I', \"'ve\"]),\n",
       " WordList([\"'ve\", 'ever']),\n",
       " WordList(['ever', 'had']),\n",
       " WordList(['had', 'I']),\n",
       " WordList(['I', \"'m\"]),\n",
       " WordList([\"'m\", 'pretty']),\n",
       " WordList(['pretty', 'sure']),\n",
       " WordList(['sure', 'they']),\n",
       " WordList(['they', 'only']),\n",
       " WordList(['only', 'use']),\n",
       " WordList(['use', 'ingredients']),\n",
       " WordList(['ingredients', 'from']),\n",
       " WordList(['from', 'their']),\n",
       " WordList(['their', 'garden']),\n",
       " WordList(['garden', 'and']),\n",
       " WordList(['and', 'blend']),\n",
       " WordList(['blend', 'them']),\n",
       " WordList(['them', 'fresh']),\n",
       " WordList(['fresh', 'when']),\n",
       " WordList(['when', 'you']),\n",
       " WordList(['you', 'order']),\n",
       " WordList(['order', 'it']),\n",
       " WordList(['it', 'It']),\n",
       " WordList(['It', 'was']),\n",
       " WordList(['was', 'amazing']),\n",
       " WordList(['amazing', 'While']),\n",
       " WordList(['While', 'EVERYTHING']),\n",
       " WordList(['EVERYTHING', 'on']),\n",
       " WordList(['on', 'the']),\n",
       " WordList(['the', 'menu']),\n",
       " WordList(['menu', 'looks']),\n",
       " WordList(['looks', 'excellent']),\n",
       " WordList(['excellent', 'I']),\n",
       " WordList(['I', 'had']),\n",
       " WordList(['had', 'the']),\n",
       " WordList(['the', 'white']),\n",
       " WordList(['white', 'truffle']),\n",
       " WordList(['truffle', 'scrambled']),\n",
       " WordList(['scrambled', 'eggs']),\n",
       " WordList(['eggs', 'vegetable']),\n",
       " WordList(['vegetable', 'skillet']),\n",
       " WordList(['skillet', 'and']),\n",
       " WordList(['and', 'it']),\n",
       " WordList(['it', 'was']),\n",
       " WordList(['was', 'tasty']),\n",
       " WordList(['tasty', 'and']),\n",
       " WordList(['and', 'delicious']),\n",
       " WordList(['delicious', 'It']),\n",
       " WordList(['It', 'came']),\n",
       " WordList(['came', 'with']),\n",
       " WordList(['with', '2']),\n",
       " WordList(['2', 'pieces']),\n",
       " WordList(['pieces', 'of']),\n",
       " WordList(['of', 'their']),\n",
       " WordList(['their', 'griddled']),\n",
       " WordList(['griddled', 'bread']),\n",
       " WordList(['bread', 'with']),\n",
       " WordList(['with', 'was']),\n",
       " WordList(['was', 'amazing']),\n",
       " WordList(['amazing', 'and']),\n",
       " WordList(['and', 'it']),\n",
       " WordList(['it', 'absolutely']),\n",
       " WordList(['absolutely', 'made']),\n",
       " WordList(['made', 'the']),\n",
       " WordList(['the', 'meal']),\n",
       " WordList(['meal', 'complete']),\n",
       " WordList(['complete', 'It']),\n",
       " WordList(['It', 'was']),\n",
       " WordList(['was', 'the']),\n",
       " WordList(['the', 'best']),\n",
       " WordList(['best', 'toast']),\n",
       " WordList(['toast', 'I']),\n",
       " WordList(['I', \"'ve\"]),\n",
       " WordList([\"'ve\", 'ever']),\n",
       " WordList(['ever', 'had']),\n",
       " WordList(['had', 'Anyway']),\n",
       " WordList(['Anyway', 'I']),\n",
       " WordList(['I', 'ca']),\n",
       " WordList(['ca', \"n't\"]),\n",
       " WordList([\"n't\", 'wait']),\n",
       " WordList(['wait', 'to']),\n",
       " WordList(['to', 'go']),\n",
       " WordList(['go', 'back'])]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.ngrams(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.stem.snowball.SnowballStemmer at 0xa467339518>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize stemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['My', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'was', 'excellent', 'The', 'weather', 'was', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', 'grounds', 'an', 'absolute', 'pleasure', 'Our', 'waitress', 'was', 'excellent', 'and', 'our', 'food', 'arrived', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', 'looked', 'like', 'the', 'place', 'fills', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', 'was', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', 'had', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', 'ingredients', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', 'was', 'amazing', 'While', 'EVERYTHING', 'on', 'the', 'menu', 'looks', 'excellent', 'I', 'had', 'the', 'white', 'truffle', 'scrambled', 'eggs', 'vegetable', 'skillet', 'and', 'it', 'was', 'tasty', 'and', 'delicious', 'It', 'came', 'with', '2', 'pieces', 'of', 'their', 'griddled', 'bread', 'with', 'was', 'amazing', 'and', 'it', 'absolutely', 'made', 'the', 'meal', 'complete', 'It', 'was', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', 'had', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back'])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'was', 'excel', 'the', 'weather', 'was', 'perfect', 'which', 'made', 'sit', 'outsid', 'overlook', 'their', 'ground', 'an', 'absolut', 'pleasur', 'our', 'waitress', 'was', 'excel', 'and', 'our', 'food', 'arriv', 'quick', 'on', 'the', 'semi-busi', 'saturday', 'morn', 'it', 'look', 'like', 'the', 'place', 'fill', 'up', 'pretti', 'quick', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'bloodi', 'mari', 'it', 'was', 'phenomen', 'and', 'simpli', 'the', 'best', 'i', 've', 'ever', 'had', 'i', \"'m\", 'pretti', 'sure', 'they', 'onli', 'use', 'ingredi', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'it', 'was', 'amaz', 'while', 'everyth', 'on', 'the', 'menu', 'look', 'excel', 'i', 'had', 'the', 'white', 'truffl', 'scrambl', 'egg', 'veget', 'skillet', 'and', 'it', 'was', 'tasti', 'and', 'delici', 'it', 'came', 'with', '2', 'piec', 'of', 'their', 'griddl', 'bread', 'with', 'was', 'amaz', 'and', 'it', 'absolut', 'made', 'the', 'meal', 'complet', 'it', 'was', 'the', 'best', 'toast', 'i', 've', 'ever', 'had', 'anyway', 'i', 'ca', \"n't\", 'wait', 'to', 'go', 'back']\n"
     ]
    }
   ],
   "source": [
    "# stem each word\n",
    "print ([stemmer.stem(word) for word in review.words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization**\n",
    "\n",
    "- **What:** Derive the canonical form ('lemma') of a word\n",
    "- **Why:** Can be better than stemming\n",
    "- **Notes:** Uses a dictionary-based approach (slower than stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['My', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'was', 'excellent', 'The', 'weather', 'was', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', 'grounds', 'an', 'absolute', 'pleasure', 'Our', 'waitress', 'was', 'excellent', 'and', 'our', 'food', 'arrived', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', 'looked', 'like', 'the', 'place', 'fills', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', 'was', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', 'had', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', 'ingredients', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', 'was', 'amazing', 'While', 'EVERYTHING', 'on', 'the', 'menu', 'looks', 'excellent', 'I', 'had', 'the', 'white', 'truffle', 'scrambled', 'eggs', 'vegetable', 'skillet', 'and', 'it', 'was', 'tasty', 'and', 'delicious', 'It', 'came', 'with', '2', 'pieces', 'of', 'their', 'griddled', 'bread', 'with', 'was', 'amazing', 'and', 'it', 'absolutely', 'made', 'the', 'meal', 'complete', 'It', 'was', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', 'had', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back'])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', u'wa', 'excellent', 'The', 'weather', u'wa', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', u'ground', 'an', 'absolute', 'pleasure', 'Our', 'waitress', u'wa', 'excellent', 'and', 'our', 'food', 'arrived', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', 'looked', 'like', 'the', 'place', u'fill', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', u'wa', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', 'had', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', u'ingredient', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', u'wa', 'amazing', 'While', 'EVERYTHING', 'on', 'the', 'menu', u'look', 'excellent', 'I', 'had', 'the', 'white', 'truffle', 'scrambled', u'egg', 'vegetable', 'skillet', 'and', 'it', u'wa', 'tasty', 'and', 'delicious', 'It', 'came', 'with', '2', u'piece', 'of', 'their', 'griddled', 'bread', 'with', u'wa', 'amazing', 'and', 'it', 'absolutely', 'made', 'the', 'meal', 'complete', 'It', u'wa', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', 'had', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back']\n"
     ]
    }
   ],
   "source": [
    "# assume every word is a noun\n",
    "print ([word.lemmatize() for word in review.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'wife', u'take', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', u'be', 'excellent', 'The', 'weather', u'be', 'perfect', 'which', u'make', u'sit', 'outside', u'overlook', 'their', u'ground', 'an', 'absolute', 'pleasure', 'Our', 'waitress', u'be', 'excellent', 'and', 'our', 'food', u'arrive', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', u'look', 'like', 'the', 'place', u'fill', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', u'be', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', u'have', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', 'ingredients', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', u'be', u'amaze', 'While', 'EVERYTHING', 'on', 'the', 'menu', u'look', 'excellent', 'I', u'have', 'the', 'white', 'truffle', u'scramble', u'egg', 'vegetable', 'skillet', 'and', 'it', u'be', 'tasty', 'and', 'delicious', 'It', u'come', 'with', '2', u'piece', 'of', 'their', u'griddle', 'bread', 'with', u'be', u'amaze', 'and', 'it', 'absolutely', u'make', 'the', 'meal', 'complete', 'It', u'be', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', u'have', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back']\n"
     ]
    }
   ],
   "source": [
    "# assume every word is a verb\n",
    "print ([word.lemmatize(pos='v') for word in review.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that accepts text and returns a list of lemmas\n",
    "def split_into_lemmas(text):\n",
    "    text = unicode(text, 'utf-8').lower()\n",
    "    words = TextBlob(text).words\n",
    "    return ([word.lemmatize() for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  16452\n",
      "Accuracy:  0.920743639922\n"
     ]
    }
   ],
   "source": [
    "# use split_into_lemmas as the feature extraction function (WARNING: SLOW!)\n",
    "vect = CountVectorizer(analyzer=split_into_lemmas)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'yuyuyummy', u'yuzu', u'z', u'z-grill', u'z11', u'zach', u'zam', u'zanella', u'zankou', u'zappos', u'zatsiki', u'zen', u'zen-like', u'zero', u'zero-star', u'zest', u'zexperience', u'zha', u'zhou', u'zia', u'zilch', u'zin', u'zinburger', u'zinburgergeist', u'zinc', u'zinfandel', u'zing', u'zip', u'zipcar', u'zipper', u'zipps', u'ziti', u'zoe', u'zombi', u'zombie', u'zone', u'zoning', u'zoo', u'zoyo', u'zucca', u'zucchini', u'zuchinni', u'zumba', u'zupa', u'zuzu', u'zwiebel-kr\\xe4uter', u'zzed', u'\\xe9clairs', u'\\xe9cole', u'\\xe9m']\n"
     ]
    }
   ],
   "source": [
    "# last 50 features\n",
    "print vect.get_feature_names()[-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example documents\n",
    "simple_train = ['call you tonight', 'Call me a cab', 'please call me... PLEASE!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     1   1       2        0    0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term Frequency\n",
    "vect = CountVectorizer()\n",
    "tf = pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    1     3   2       1        1    1"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Document Frequency\n",
    "vect = CountVectorizer(binary=True)\n",
    "df = vect.fit_transform(simple_train).toarray().sum(axis=0)\n",
    "pd.DataFrame(df.reshape(1, 6), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab      call   me  please  tonight  you\n",
       "0  0.0  0.333333  0.0     0.0      1.0  1.0\n",
       "1  1.0  0.333333  0.5     0.0      0.0  0.0\n",
       "2  0.0  0.333333  0.5     2.0      0.0  0.0"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term Frequency-Inverse Document Frequency (simple version)\n",
    "tf/df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.652491</td>\n",
       "      <td>0.652491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.720333</td>\n",
       "      <td>0.425441</td>\n",
       "      <td>0.547832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266075</td>\n",
       "      <td>0.342620</td>\n",
       "      <td>0.901008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cab      call        me    please   tonight       you\n",
       "0  0.000000  0.385372  0.000000  0.000000  0.652491  0.652491\n",
       "1  0.720333  0.425441  0.547832  0.000000  0.000000  0.000000\n",
       "2  0.000000  0.266075  0.342620  0.901008  0.000000  0.000000"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TfidfVectorizer\n",
    "vect = TfidfVectorizer()\n",
    "pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More details:** [TF-IDF is about what matters](http://planspace.org/20150524-tfidf_is_about_what_matters/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Using TF-IDF to Summarize a Yelp Review\n",
    "\n",
    "Reddit's autotldr uses the [SMMRY](http://smmry.com/about) algorithm, which is based on TF-IDF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TfidfVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28880)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a document-term matrix using TF-IDF\n",
    "vect = TfidfVectorizer(stop_words='english')\n",
    "dtm = vect.fit_transform(yelp.text)\n",
    "features = vect.get_feature_names()\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summarize():\n",
    "    \n",
    "    # choose a random review that is at least 300 characters\n",
    "    review_length = 0\n",
    "    while review_length < 300:\n",
    "        review_id = np.random.randint(0, len(yelp))\n",
    "        review_text = unicode(yelp.text[review_id], 'utf-8')\n",
    "        review_length = len(review_text)\n",
    "    \n",
    "    # create a dictionary of words and their TF-IDF scores\n",
    "    word_scores = {}\n",
    "    for word in TextBlob(review_text).words:\n",
    "        word = word.lower()\n",
    "        if word in features:\n",
    "            word_scores[word] = dtm[review_id, features.index(word)]\n",
    "    \n",
    "    # print words with the top 5 TF-IDF scores\n",
    "    print 'TOP SCORING WORDS:'\n",
    "    top_scores = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    for word, score in top_scores:\n",
    "        print word\n",
    "    \n",
    "    # print 5 random words\n",
    "    print '\\n' + 'RANDOM WORDS:'\n",
    "    random_words = np.random.choice(word_scores.keys(), size=5, replace=False)\n",
    "    for word in random_words:\n",
    "        print word\n",
    "    \n",
    "    # print the review\n",
    "    print '\\n' + review_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP SCORING WORDS:\n",
      "pushes\n",
      "cheesey\n",
      "processing\n",
      "greedy\n",
      "discouraged\n",
      "\n",
      "RANDOM WORDS:\n",
      "cheesey\n",
      "work\n",
      "fat\n",
      "sitting\n",
      "thing\n",
      "\n",
      "This place pushes out some delicious food, Red chile burro with rice and beans is better than any I've had, and their Nachos are soooo freaking cheesey and good you'll want to order both for one sitting. Unfortunately only outside seating but cool art work on the building to enjoy while eating and don't be discouraged by there greedy practice of charging for extra hot sauce. But of all things to remember make sure you bring cash the place can't afford a card processing machine which is a good thing other wise I'd be there too often and a fat face looks weird on me\n"
     ]
    }
   ],
   "source": [
    "summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\n",
      "\n",
      "Do yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\n",
      "\n",
      "While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\n",
      "\n",
      "Anyway, I can't wait to go back!\n"
     ]
    }
   ],
   "source": [
    "print review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40246913580246907"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# polarity ranges from -1 (most negative) to 1 (most positive)\n",
    "review.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  length  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0     889  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# understanding the apply method\n",
    "yelp['length'] = yelp.text.apply(len)\n",
    "yelp.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that accepts text and returns the polarity\n",
    "def detect_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new DataFrame column for sentiment (WARNING: SLOW!)\n",
    "yelp['sentiment'] = yelp.text.apply(detect_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.402469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.229773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.608646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.468125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  sentiment  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0   0.402469  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0   0.229773  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0   0.566667  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0   0.608646  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0   0.468125  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xf4e84a8>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEcCAYAAAAGD4lRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCEFKGQsdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f16ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# box plot of sentiment grouped by stars\n",
    "yelp.boxplot(column='sentiment', by='stars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254    Our server Gary was awesome. Food was amazing....\n",
       "347    3 syllables for this place. \\nA-MAZ-ING!\\n\\nTh...\n",
       "420                                    LOVE the food!!!!\n",
       "459    Love it!!! Wish we still lived in Arizona as C...\n",
       "679                                     Excellent burger\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reviews with most positive sentiment\n",
    "yelp[yelp.sentiment == 1].text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "773     This was absolutely horrible. I got the suprem...\n",
       "1517                  Nasty workers and over priced trash\n",
       "3266    Absolutely awful... these guys have NO idea wh...\n",
       "4766                                       Very bad food!\n",
       "5812        I wouldn't send my worst enemy to this place.\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reviews with most negative sentiment\n",
    "yelp[yelp.sentiment == -1].text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# widen the column display\n",
    "pd.set_option('max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390     RIP AZ Coffee Connection.  :(  I stopped by two days ago unaware that they had closed.  I am severely bummed.  This place is irreplaceable!  Damn you, Starbucks and McDonalds!\n",
      "1287                                             Obsessed. Like, I've-got-the-Twangy-Tart-withdrawal-shakes level of addiction to this place. Please make one in Arcadia! Pleeeaaassse.\n",
      "3075                                                                                                                                                     Unfortunately Out of Business.\n",
      "3516                                                                                                      Cashew brittle, almond brittle, bacon brittle!  Go now, before it's too late!\n",
      "6726                                                                                            Brown bag chicken sammich, mac n cheese, fried okra, and the bourbon drink.  Nuff said.\n",
      "9809                                                             I have to tell you....\\n\\nI had their Jerk Chicken Plate the other night, and it was yuuuuummmmmyyy! You gotta try it!\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# negative sentiment in a 5-star review\n",
    "print yelp[(yelp.stars == 5) & (yelp.sentiment < -0.3)].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1781                                                                                                                                                                                                                                                                 If you like the stuck up Scottsdale vibe this is a good place for you. The food isn't impressive. Nice outdoor seating.\n",
      "2353    My co-workers and I refer to this place as \"Pizza n' Ants\".  The staff will be happy to serve you with bare hands, right after using the till.  Also, as the nickname suggests, there has been a noticable insect problem. \\n\\n\\n\\nAs if that could all be overlooked, the pizza isn't even good.  If you are in this part of town, go to Z Pizza or Slices for great pizza instead!\n",
      "5257                                                                        Remember how I said that the Trivia was the best thing about this place?  Well, they got rid of long time Triva host, Dave (who had been featured in the College Times and was the best thing about the trivia).  Without Dave's personality, this place just doesn't cut it.  Will never go here again. Bummer.\n",
      "6222                                                                                                                                                                                                                                                                                                           My mother always told me, if I didn't have anything nice to say, say nothing!\n",
      "6702                                                                                                                                                                         Most livable city my eye!\\nPlastic yuppies around every corner looking for a reason to belong.  I can't wait for the homosexuals to take control of this dog park and give it some class.\\n\\nAvoid at all cost.\n",
      "8833                                                                                                                                                                                                                                   The owner has changed hands & this place isn't what it used to be.  If you want up to date paper & quality product...go to Scrap Happy OR Crop Girls!\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# positive sentiment in a 1-star review\n",
    "print yelp[(yelp.stars == 1) & (yelp.sentiment > 0.5)].text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset the column display width\n",
    "pd.reset_option('max_colwidth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Adding Features to a Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a DataFrame that only contains the 5-star and 1-star reviews\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "\n",
    "# define X and y\n",
    "feature_cols = ['text', 'sentiment', 'cool', 'useful', 'funny']\n",
    "X = yelp_best_worst[feature_cols]\n",
    "y = yelp_best_worst.stars\n",
    "\n",
    "# split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3064, 16825)\n",
      "(1022, 16825)\n"
     ]
    }
   ],
   "source": [
    "# use CountVectorizer with text column only\n",
    "vect = CountVectorizer()\n",
    "X_train_dtm = vect.fit_transform(X_train.text)\n",
    "X_test_dtm = vect.transform(X_test.text)\n",
    "print X_train_dtm.shape\n",
    "print X_test_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 4)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of other four feature columns\n",
    "X_train.drop('text', axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 4)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cast other feature columns to float and convert to a sparse matrix\n",
    "extra = sp.sparse.csr_matrix(X_train.drop('text', axis=1).astype(float))\n",
    "extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 16829)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine sparse matrices\n",
    "X_train_dtm_extra = sp.sparse.hstack((X_train_dtm, extra))\n",
    "X_train_dtm_extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1022, 16829)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeat for testing set\n",
    "extra = sp.sparse.csr_matrix(X_test.drop('text', axis=1).astype(float))\n",
    "X_test_dtm_extra = sp.sparse.hstack((X_test_dtm, extra))\n",
    "X_test_dtm_extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.917808219178\n"
     ]
    }
   ],
   "source": [
    "# use logistic regression with text column only\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train_dtm, y_train)\n",
    "y_pred_class = logreg.predict(X_test_dtm)\n",
    "print metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.922700587084\n"
     ]
    }
   ],
   "source": [
    "# use logistic regression with all features\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train_dtm_extra, y_train)\n",
    "y_pred_class = logreg.predict(X_test_dtm_extra)\n",
    "print metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Fun TextBlob Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"15 minutes late\")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spelling correction\n",
    "TextBlob('15 minuets late').correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('part', 0.9929478138222849), (u'parrot', 0.007052186177715092)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spellcheck\n",
    "Word('parot').spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'tip laterally',\n",
       " u'enclose with a bank',\n",
       " u'do business with a bank or keep an account at a bank',\n",
       " u'act as the banker in a game or in gambling',\n",
       " u'be in the banking business',\n",
       " u'put into a bank account',\n",
       " u'cover with ashes so to control the rate of burning',\n",
       " u'have confidence or faith in']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# definitions\n",
    "Word('bank').define('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'es'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# language identification \n",
    "TextBlob('Hola amigos').detect_language()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
